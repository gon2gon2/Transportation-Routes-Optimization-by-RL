{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 3\n",
    "T_horizon     = 20\n",
    "\n",
    "def huber(x, k=1.0):\n",
    "    return torch.where(x.abs() < k, 0.5 * x.pow(2), k * (x.abs() - 0.5 * k))\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, state_dim,action_dim,quantile_num,learning_rate):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.quantile_num = quantile_num\n",
    "        \n",
    "        super(Agent,self).__init__()\n",
    "        self.memory = []\n",
    "\n",
    "        self.fc1 = nn.Linear(self.state_dim,256)\n",
    "        self.policy = nn.Linear(256, self.action_dim)\n",
    "        self.value = nn.Linear(256, self.action_dim * self.quantile_num)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr = learning_rate)\n",
    "        \n",
    "    def get_action(self,x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.policy(x)\n",
    "        prob = F.softmax(x, dim = softmax_dim)\n",
    "        return prob\n",
    "    \n",
    "    def get_value(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.value(x)\n",
    "        x = x.view(-1,self.action_dim, self.quantile_num)\n",
    "        return x\n",
    "    \n",
    "    def put_data(self,data):\n",
    "        self.memory.append(data)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        state_list, action_list, reward_list, next_state_list, prob_list, done_list = [],[],[],[],[],[]\n",
    "        for data in self.memory:\n",
    "            state,action,reward,next_state,prob,done = data\n",
    "            state_list.append(state)\n",
    "            action_list.append([action])\n",
    "            reward_list.append([reward])\n",
    "            prob_list.append([prob])\n",
    "            next_state_list.append(next_state)\n",
    "            done_mask = 0 if done else 1\n",
    "            done_list.append([done_mask])\n",
    "        self.memory = []\n",
    "        \n",
    "        s,a,r,next_s,done_mask,prob = torch.tensor(state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(action_list),torch.tensor(reward_list),\\\n",
    "                                        torch.tensor(next_state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(done_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(prob_list)\n",
    "        return s,a,r,next_s,done_mask,prob\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        state,action,reward, next_state,done_mask,action_prob = self.make_batch()\n",
    "        \n",
    "        for i in range(K_epoch):\n",
    "            next_get_value = self.get_value(next_state)[np.arange(state.shape[0]),action.reshape(-1,)].detach()\n",
    "            now_get_value = self.get_value(state)[np.arange(state.shape[0]),action.reshape(-1,)]\n",
    "\n",
    "            diff = next_get_value.reshape(state.shape[0],self.quantile_num,1).\\\n",
    "                        repeat(1,1,self.quantile_num) \\\n",
    "                    - now_get_value.repeat(1,5).view(-1,5,5)\n",
    "            \n",
    "            value_loss = huber(diff) * (tau - (diff.detach()<0).float()).abs()\n",
    "            value_loss = value_loss.mean(-1).mean(-1)\n",
    "            \n",
    "            td_error = reward + gamma * next_get_value.mean(-1) * done_mask\n",
    "            \n",
    "            delta = td_error - now_get_value.mean(-1)\n",
    "            #delta = torch.mean(delta,1).reshape(-1,1)\n",
    "            delta = delta.detach().numpy()\n",
    "            advantage_list = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_list.append([advantage])\n",
    "            advantage_list.reverse()\n",
    "            advantage = torch.tensor(advantage_list,dtype = torch.float)\n",
    "            \n",
    "            \n",
    "            now_action = self.get_action(state,softmax_dim = 1)\n",
    "            now_action = now_action.gather(1,action)\n",
    "            \n",
    "            ratio = torch.exp(torch.log(now_action) - torch.log(action_prob))\n",
    "            \n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio , 1-eps_clip, 1 + eps_clip) * advantage\n",
    "            \n",
    "            \n",
    "            loss = (-torch.min(surr1,surr2)).mean(-1)\n",
    "\n",
    "            loss = (loss + 10*value_loss).mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from Building import Building\n",
    "#from Agent import Agent\n",
    "import time\n",
    "#====================================================================================\n",
    "\n",
    "\n",
    "#====================================================================================\n",
    "#Building Setting\n",
    "lift_num = 1\n",
    "buliding_height = 5\n",
    "max_people_in_floor = 8\n",
    "max_people_in_elevator = 10\n",
    "\n",
    "add_people_at_step = 25\n",
    "add_people_prob = 0.8\n",
    "\n",
    "#Create building with 4 elevators, height 10, max people 30 in each floor\n",
    "building = Building(lift_num, buliding_height, max_people_in_floor,max_people_in_elevator)\n",
    "\n",
    "#Agent controls each elevator\n",
    "#agent = Agent(buliding_height, lift_num, 4)\n",
    "#agent.reload(280)\n",
    "#The goal is to bring down all the people in the building to the ground floor\n",
    "\n",
    "epochs = 1000\n",
    "max_steps = 100\n",
    "global_step = 0\n",
    "T_horizon = 20\n",
    "reward_list = []\n",
    "print_interval = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = 4\n",
    "quantile_num = 5\n",
    "model = Agent((buliding_height)+ max_people_in_elevator + (lift_num *2),action_dim,quantile_num,learning_rate)\n",
    "print_interval = 20\n",
    "ave_reward = 0 \n",
    "tau = torch.Tensor((2 * np.arange(model.quantile_num) + 1) / (2.0 * model.quantile_num)).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 170.2\n",
      "quantile value bar tensor([[[ 0.0674,  0.0838,  0.0896,  0.0823,  0.1084],\n",
      "         [ 0.0429,  0.0593,  0.0706,  0.0567,  0.0447],\n",
      "         [-0.0185,  0.0027,  0.0008, -0.0073,  0.0207],\n",
      "         [-0.0126, -0.0128,  0.0067, -0.0158,  0.0250]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :40, avg score : 229.0\n",
      "quantile value bar tensor([[[ 3.9137e-01,  3.8561e-01,  3.8620e-01,  3.9266e-01,  3.8972e-01],\n",
      "         [ 3.8674e-01,  4.0508e-01,  3.9566e-01,  4.1906e-01,  4.4535e-01],\n",
      "         [-7.8571e-02, -7.5463e-02, -7.2863e-02, -7.1602e-02, -7.1615e-02],\n",
      "         [-1.0630e-02, -6.6116e-03, -9.4353e-03, -4.6306e-03,  4.3338e-04]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :60, avg score : 209.3\n",
      "quantile value bar tensor([[[ 0.3400,  0.3328,  0.3426,  0.3370,  0.3397],\n",
      "         [ 0.4277,  0.4509,  0.4685,  0.4709,  0.4942],\n",
      "         [-0.1827, -0.1830, -0.1808, -0.1819, -0.1850],\n",
      "         [ 0.0535,  0.0380,  0.0565,  0.0572,  0.0707]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :80, avg score : 244.8\n",
      "quantile value bar tensor([[[ 0.8332,  0.8517,  0.8571,  0.8626,  0.8617],\n",
      "         [ 1.4847,  1.5684,  1.5677,  1.6277,  1.6490],\n",
      "         [-0.4616, -0.4794, -0.4585, -0.4330, -0.4292],\n",
      "         [ 0.0656,  0.0641,  0.0594,  0.0938,  0.0892]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :100, avg score : 251.2\n",
      "quantile value bar tensor([[[ 0.9496,  0.9544,  0.9568,  0.9587,  0.9595],\n",
      "         [ 2.4531,  2.5251,  2.5394,  2.5679,  2.5863],\n",
      "         [-0.7501, -0.7527, -0.7444, -0.7504, -0.7481],\n",
      "         [ 0.1003,  0.1006,  0.1002,  0.1045,  0.1079]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :120, avg score : 232.1\n",
      "quantile value bar tensor([[[ 1.0633,  1.0657,  1.0674,  1.0678,  1.0743],\n",
      "         [ 3.4839,  3.6007,  3.6179,  3.6584,  3.6850],\n",
      "         [-1.0577, -1.0571, -1.0508, -1.0536, -1.0519],\n",
      "         [ 0.1379,  0.1379,  0.1357,  0.1367,  0.1371]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :140, avg score : 293.2\n",
      "quantile value bar tensor([[[ 2.0547,  2.0650,  2.0697,  2.0735,  2.0797],\n",
      "         [ 7.7289,  7.9424,  7.9428,  8.0481,  8.1094],\n",
      "         [-2.2917, -2.2761, -2.2721, -2.2756, -2.2613],\n",
      "         [ 0.1921,  0.1972,  0.1927,  0.2010,  0.2041]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :160, avg score : 262.6\n",
      "quantile value bar tensor([[[ 3.2213,  3.2126,  3.2356,  3.2327,  3.2386],\n",
      "         [19.3352, 19.8863, 19.9063, 20.0849, 20.2191],\n",
      "         [-5.0577, -5.0572, -5.0550, -5.0527, -5.0512],\n",
      "         [ 0.1326,  0.1270,  0.1352,  0.1381,  0.1431]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :180, avg score : 297.3\n",
      "quantile value bar tensor([[[ 2.3513,  2.3601,  2.3634,  2.3283,  2.3681],\n",
      "         [28.5992, 30.0782, 30.2268, 30.8112, 31.0778],\n",
      "         [-7.0873, -7.1007, -7.0911, -7.1025, -7.1316],\n",
      "         [ 0.0886,  0.0480,  0.0548,  0.0867,  0.0867]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :200, avg score : 292.0\n",
      "quantile value bar tensor([[[ 1.1366,  1.1392,  1.1412,  1.1468,  1.1489],\n",
      "         [21.6009, 22.4984, 22.8277, 23.1286, 23.3224],\n",
      "         [-5.8320, -5.7707, -5.7515, -5.7295, -5.6842],\n",
      "         [ 0.0918,  0.0989,  0.1034,  0.1074,  0.1051]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :220, avg score : 261.0\n",
      "quantile value bar tensor([[[ 1.6900e+00,  1.6870e+00,  1.6783e+00,  1.6905e+00,  1.7255e+00],\n",
      "         [ 6.9396e+01,  7.0403e+01,  7.0983e+01,  7.1568e+01,  7.2326e+01],\n",
      "         [-1.5030e+01, -1.5023e+01, -1.5009e+01, -1.4997e+01, -1.4944e+01],\n",
      "         [ 5.5437e-02,  4.3179e-02,  4.5718e-02,  4.9828e-02,  5.8965e-02]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :240, avg score : 301.0\n",
      "quantile value bar tensor([[[ 1.8755e+00,  2.2107e+00,  1.8839e+00,  2.1076e+00,  2.1922e+00],\n",
      "         [ 1.2322e+02,  1.2362e+02,  1.2373e+02,  1.2386e+02,  1.2415e+02],\n",
      "         [-2.4145e+01, -2.4119e+01, -2.4117e+01, -2.4105e+01, -2.4075e+01],\n",
      "         [-9.7383e-02, -4.3714e-02,  7.6629e-02, -2.8053e-02,  5.8895e-02]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :260, avg score : 301.0\n",
      "quantile value bar tensor([[[ 2.4566e+00,  2.4622e+00,  2.4743e+00,  2.4585e+00,  2.5303e+00],\n",
      "         [ 1.6970e+02,  1.6984e+02,  1.6990e+02,  1.6988e+02,  1.7015e+02],\n",
      "         [-3.2361e+01, -3.2355e+01, -3.2357e+01, -3.2357e+01, -3.2349e+01],\n",
      "         [-6.0964e-01, -3.0163e-01, -5.0300e-01, -1.2514e-01, -1.4328e-01]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :280, avg score : 301.0\n",
      "quantile value bar tensor([[[ 2.0772e+00,  2.1176e+00,  2.1259e+00,  2.0847e+00,  2.1268e+00],\n",
      "         [ 1.6794e+02,  1.6855e+02,  1.6871e+02,  1.6882e+02,  1.6898e+02],\n",
      "         [-3.2089e+01, -3.2090e+01, -3.2078e+01, -3.2073e+01, -3.2066e+01],\n",
      "         [-8.8733e-03,  1.3471e-02,  2.0089e-01,  4.1585e-01,  5.2704e-01]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :300, avg score : 301.0\n",
      "quantile value bar tensor([[[ 1.7237e+00,  2.1412e+00,  2.0965e+00,  2.1413e+00,  2.3629e+00],\n",
      "         [ 1.8465e+02,  1.8469e+02,  1.8471e+02,  1.8471e+02,  1.8474e+02],\n",
      "         [-3.6082e+01, -3.6079e+01, -3.6094e+01, -3.6102e+01, -3.6103e+01],\n",
      "         [ 1.6171e-01, -1.1875e-01, -3.8530e-01,  3.8715e-01,  6.0433e-01]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "# of episode :320, avg score : 301.0\n",
      "quantile value bar tensor([[[ 2.7947e+00,  2.6135e+00,  2.9117e+00,  2.9172e+00,  3.0647e+00],\n",
      "         [ 2.9531e+02,  2.9566e+02,  2.9552e+02,  2.9541e+02,  2.9573e+02],\n",
      "         [-5.4254e+01, -5.4210e+01, -5.4194e+01, -5.4177e+01, -5.4121e+01],\n",
      "         [-1.1108e+00, -4.0755e-01,  4.2803e-01,  2.2179e-01,  4.7283e-01]]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-dfc11c4d986d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mave_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m#print(\"Epoch: %d Step: %d Average Reward: %.4f\"%(epoch, global_step, ave_reward/global_step))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-139-0428b3344ee1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\unity\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    building.empty_building()\n",
    "    while building.target == 0 :\n",
    "        building.generate_people(add_people_prob)\n",
    "    state = building.get_state()\n",
    "    done = False\n",
    "    global_step = 0\n",
    "    while not done:\n",
    "        for t in range(T_horizon):\n",
    "            global_step += 1\n",
    "            if (global_step % 25 == 0) & global_step > 0 :\n",
    "                #building.generate_people(add_people_prob/2)\n",
    "                pass\n",
    "            prev_people = building.get_arrived_people()\n",
    "            action_prob = model.get_action(torch.from_numpy(np.array(state)).float())\n",
    "            m = Categorical(action_prob)\n",
    "            action = m.sample().item()\n",
    "            building.perform_action([action])\n",
    "            reward = building.get_reward(prev_people) \n",
    "            \n",
    "            next_state = building.get_state()\n",
    "            finished = next_state.copy()\n",
    "            del finished[5:7]\n",
    "            if (sum(finished) == 0.0) :\n",
    "                reward = 100\n",
    "                done = True\n",
    "            model.put_data((state, action, reward/10.0, next_state, action_prob[action].item(), done))\n",
    "            state = next_state\n",
    "            \n",
    "            if done or global_step > 300:\n",
    "                done = True\n",
    "                break\n",
    "        model.train()\n",
    "    ave_reward += global_step \n",
    "    #print(\"Epoch: %d Step: %d Average Reward: %.4f\"%(epoch, global_step, ave_reward/global_step))\n",
    "    if epoch%print_interval==0 and epoch!=0:\n",
    "        print(\"# of episode :{}, avg score : {:.1f}\".format(epoch, ave_reward/print_interval))\n",
    "        print('quantile value bar',model.get_value(torch.tensor(next_state)))\n",
    "        ave_reward = 0\n",
    "    if (epoch % 100 == 0 )& (epoch != 0):\n",
    "        torch.save(model.state_dict(), './model_weights/quantile_model_'+str(epoch))\n",
    "    reward_list.append(global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity",
   "language": "python",
   "name": "unity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
